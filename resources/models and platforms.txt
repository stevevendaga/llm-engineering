openrouter.ai
model GLM 4.7
usage: 
response = openrouter.chat.completions.create(model="z-ai/glm-4.7", messages=[{"role": "user", "content": "Tell a joke about cats."}])

Abstraction layers
Langchain vs LiteLLM

Langchain: 
- Langchain is a popular library for building AI applications.
 It provides a high-level interface for working with LLMs, making it easier to use and more accessible for developers. 
 Langchain supports a variety of LLMs, including OpenAI, Hugging Face, and Cohere. It also provides a variety of tools for working with LLMs, 
 such as prompt engineering, data preprocessing, and evaluation. Langchain is a good choice for building applications that require a high level of abstraction.

Usage
from langchain_openai import ChatOpenAI

llm =ChatOpenAI(model="gpt-5-mini")
response=llm.invoke(tell_a_joke)
display(Markdown(response))

LiteLLM  is a lightweight LLM library that supports a variety of LLMs, including OpenAI, Hugging Face, and Cohere.
It also provides a variety of tools for working with LLMs, such as prompt engineering, data preprocessing, and evaluation.

Usage
from litellm import completion
response = completion(model="openai/gpt-4.1", messages=[{"role": "user", "content": "tell_a_joke"}])
display(Markdown(response))

Gemini
gemini/gemini-2.5-flash-lite

Research prompt caching

Gradio from HuggingFace
Building UIs with Gradio
Gradio is an open-source Python library that allows you to build UIs for your machine learning models. 
It's designed to be easy to use and integrate into your existing projects.

Gradio Resources
https://www.gradio.app/guides/quickstart

=> Tools
- Allows frontier model to connect with external functions
- Richer responses by extending knowledge
- Ability to carry out actions within the applications
- Enhanced capabilities, like calculations

Common use cases
1. fetch data or add knowledge or context
2. take action, like booking a meeting
3. perform calculations
4. modify the UI
and two other ways to use tools that forms the basis of Agentic AI
1. a tool can be used to make another call to an LLM
2. A tool can be used to track a ToDo list and track progress towards a goal

Agents (Agentic AI)
2 definitions
-> LLM that controls the workflow
-> An LLM agent that runs tools in a loop to achieve a goal
Common features
-> Memory/persistence
-> Planning capabilities
-> Autonomy
-> LLM orchestration via Tools
-> Functionality via Tools

Types of Gradio UI
1. gr.interface is for standard, simple UIs
2. gr.ChatInterface is for standard Chatbot UIs
3. gr.Blocks is for custom UIs where you control the components and the callbacks

Hugging Face
Two parts of hugging Face
1. Hugging face platform - huggingface.co
    - Hugging face model hub (open source models of all shapes and sizes)
    - Hugging face datasets (a treasure trove of datasets)
    - Hugging face spaces (a platform for building and sharing ML apps)
        - apps, many built in gradio, including leaderboards

2. Hugging Face Libraries
- Hugging face has open source libraries and implements open source transformer models using 
 a. pytorch (most popular)
 b  tensorflow

Six Hugging face libraries
1. hub (its a python library that lets you connect to hugging face hub to download the models or datasets from hugging face)
2. datasets
3. transformers 
4. peft (Paremeter Efficient Fine-Tuning)
5. trl (Transformer Reinforcement Learning)
6. accelerate (its a python library that lets you train your models on multiple GPUs or TPUs)

Google Colab
https://colab.research.google.com/
https://colab.research.google.com/drive/1DjcrYDZldAXKJ08x1uYIVCtItoLPk1Wr?usp=sharing

Code on a powerful GPU box
- run a Jupyter notebook in the cloud with a powerful runtime
- collaborate with others
- integrate with other Google services

Runtimes
Change runtime type to T4 GPU
1. CPU based
2. Lower spec GPU for free or low-cost T4(15GB GPU RAM)
3. Higher spec GPU for resource intensive runs - A100(40GB)

Setting up Colab with Hugging Face and running a model
-> Connecting Hugging Face to Colab
Go to https://huggingface.co/
-> Click on your profile icon in the top right corner, profile
-> Click on "Settings"
-> Access Tokens
-> Click on "Create new token"
-> Select Write in Token type
-> Give it a name and click on "Create token"
-> Copy the token and paste it in the Colab notebook
-> In colab, click on secrets, add new secret
-> Name it HF_TOKEN

Accessing the secret key
from google.colab import userdata
userdata.get('secretName') where secretName is HF_TOKEN


Introduction to Hugging Face Pipelines for quick AI inference
-> The two API level of Hugging Face
1. Pipelines: Higher level APIs to carry out standard tasks incredibly quickly. Pipelines are for simple out-of-the-box inference task.
 E.g.
    1. Sentiment analysis
    2. Classifier
    3. Named Entity Recognition
    4. Question Answering
    5. Summaring
    6. Translation

2. Tokenizers and Models: Lower level APIs to provide the most power and control

Use Pipelines to generate content (text, image, audio)
Training and inference
1. Training is when you provide a model with data for it to adapt to get better at a task in the future. it does this by updating its internal settings - the parameters or weights of the model.
    If you are Training a model that's already had some training, the activity is called fine-tuning.
2. Inference is when you are working with a model that has already been trained. You are using that model to produce new outputs on new inputs, taking advantage of everthing it learned while it was being trained.
    Inference is also sometimes referred to as "Execution" or "Running a model".
    The pipelines API in HuggingFace is only for use for inference - running a model that has already been trained.

Using Pipelines from Hugging Face
Step 1: Create a pipeline function then call

my_pipeline = pipeline(task, model =xx, device =xx)

device = "cuda" for NVIDIA GPU like T4, mps on a mac

Step 2: Then call it as many times as you want

my_pipeline(input1)
my_pipeline(input2)

Tokenizers: How LLMs converts text to numbers
Tokenizers are a bit of code that maps between Text and Tokens for a particular model
-> Translates between Text, Tokens and IDs with encode() and decode() methods
-> Contains a dictionary that can include special tokens to signal information to LLM, like start of prompt
-> Contains special tokens like start of a prompt (number 10 can be configured to indicate the start of a prompt)

The Tokenenizers for key models
Meta - Llama 3.1
Microsoft's entrant - Phi
DeepSeekAI - DeepSeek 3.1
Leading open-source coding model - Qwen 2.5 Coder

Tokenizers in action: Encoding and Decoding with Llama3.1

Pytorch is a python library for writing neural networks.

=> Choosing the Right LLM for your task 
Compare the following features of an LLM
The basics 1
1. Open-source or closed
2. Chat/Reasoning/Hybrid
3. Release date and knowledge cut-off
4. Paremeters
5. Training tokens
6. Context window

The basics 2
1. Inference cost
2. Training cost
3. Build cost
4. Time to market
5. Rate limits
6. Speed
7. Latency
8. Licence

=> AI leaderboards
1. Artificial analysis (artificialanalysis.ai)
2. Vellum
3. Scale.com (SEAL leaderboards)
4. Hugging Face leaderboards

=> RAG (Retrieval Augmented Generation)
Build a knowledge base - a database of useful information
1. Search the knowledge base for relevant information
2. Use the relevant information to generate a response
RAG is a pattern for building LLM applications that use a knowledge base to generate responses.

=> Vector embeddings and Encoder LLMs: The foundation of RAG
Encoding LLMs and Vector embeddings

Auto-Encoding vs Auto-Regressive LLMs
- Auto-regressive LLMs predict a future token based on the previous tokens
- Auto-encoding produce output based on the full input

Auto-encoding LLMs
- Applications include Sentiment Analysis and Classification
- Also used to calculate "Vector Embeddings",  representing an input as a list of numbers i.e. a Vector
    Examples include BERT (Bidirectional Encoding Representations from Transformers) from Google and OpenAI Embeddings from OpenAI

Difference between Tokens and Vectors
Token is the numerical representation of the input,
while vectors are the numerical representations of those words and are outputs

These vectors mathematically represent the 'meaning' of an input
- can represent a character, a token, a word, an entire document, or something abstraction
- typically have hundreds, or thousands of dimensions
- represent an 'understanding' of the inputs; similar inputs are close to each other
- support 'vector math' like the famous example 
    "King - Man + Woman = Queen"

Vector embeddings is simply a set of numbers  that represent a word

=> The big idea behind RAG
Question <-> Code <-> Encoder (Encoding LLM)
Code <-> Vector Datastore
Code -> LLM
The question is encoded into a vector by the encoder, which is used to retrieve the most relevant information from the vector datastore.
Then the retrieved information is passed to the LLM to answer the question.

=> Introduction to Langchain and Vector Databases
LangChain
- Open Source framework launched in October 2022
- Provides a common framework for interfacing with many LLMs
- LangChain v1 released October 2025 with sifnificant changes

Pros
- Simplifies the creation of applications using LLMs (eg AI assistants, RAG, summarization)
- sifnificant adoption in enterprise

Cons
- As APIs for LLMs have matured, converged and simplified, the need for a unifying framework like LangChain has decreased - particulary with OpenAI endpoins
- LangChain is a more heavyweight abstraction layer than LiteLLM and others and some of its code can be 'legacy'

=> Breaking documents into chunks with LangChain

=> Encoder Models vs Vector Databases: OpenAI, BERT, Chroma & FAISS

Encoder/Embedding Models

The Encoder Model turns text into a Vector Embedding.
This is then stored in a Vector Database like Chroma

Encoder Models
1. word2vec  (2013)
2. BERT (2018) from google

Modern Embedding Models
3. OpenAI
    - text embedding 3 small
    - text embedding 3 large
4. Google
    - gemini-embedding-001
5. Hugging Face Sentence Transformers
    - all-MiniLM-L6-v2

=> Use LangChain to load our Knowledge Base
- Read in the documents in all folders
- Divide the documents into chunks
- Store the chunks in a Vector Database
- Create a Retrieval Chain to query the Vector Database

=> Popular Vectorestores
Open-Source: Chroma, Qdrant, FAISS (in-memory)
Paid & scalable: Pinecone, Weaviate, etc
Mainstream databases (Postgres, MongoDB, Elastic)

=> Creating Vector Stores with Chroma and visualizing embeddings with t-SNE (t-distributed stochastic neighbor embedding)
=> Vector Visualizations and Comparing Embedding Models

=> Building a Complete RAG pipeline with LangChain and Chroma

Two key abstractions
1. LLm 
2. Retrieval
Many common abstraction like this can be called with invoke()

temperature as a parameter
temperature is a parameter that controls the randomness of the output. It is a value between 0 and 1, where 0 is deterministic and 1 is completely random. 
A higher temperature will result in more diverse outputs, while a lower temperature will result in more focused outputs.

=> Building RAG with LangChain: Retriever and LLM Integration
=> Building Production RAG with Python Modules and Gradio UI
=> RAG with Conversation History: Building a Gradio UI and Debugging Chunki

=>> Convert Documents Into LLM-Ready Markdown
1. MarkItDown library
2. Pandoc  library

See materials-python-markitdown for more details.

==>> 10 RAG Advanced Techniques for optimizing Retrieval-Augmented Generation (RAG) processes. The techniques include:
(Semantic Chunking)
1. Chunking R&D: Experimenting with chunking strategies.
2. Encoder R&D: Selecting the best encoder model using a test set.
3. Improve Prompts: Enhancing general content with current data, context, and history.
4. Document Pre-processing: Utilizing a Language Learning Model (LLM) for chunking and text encoding.
5. Query Rewriting: Converting user questions to RAG queries with LLM.
6. Query Expansion: Using LLM to create multiple RAG queries from one question.
7. Re-ranking: Employing LLM to refine RAG results.
8. Hierarchical: Summarizing at multiple levels with LLM.
9. Graph RAG: Retrieving content related to similar documents.
10. Agentic RAG: Using agents for retrieval, integrating memory and tools like SQL.

=>> Fine-tuning LLMs:

Curating datasets
Data is typically divided into 3 datasets
1. Training Data
2. Validation Data
3. Test Data

=>> Five step strategy for selecting, training and applying an LLM

1. Understand the problem
2. Prepare
3. Select
4. Customize
5. Productionize

>> Understand
- Gather business requirements
- Identify performance criteria
- understand the data: quantity, quality, format
- Determine non-functionals
 Cost, scalability, latency

>> Prepare
- Research existing/non-LLM solutions - Potential baseline model
- Compare relevant LLMs
 The basics, context length, price, license, leaderboards, benchmarks
- Curate data

>> Select
- Choose LLM (s)
- Experiment
- Train and validate with curated data

>> Customize
Four techniques to optimize the performance of the model

> Inference Time
1. Prompt (e.g multi-shot prompting)
2. RAG
3. Agents

> Training Time
4. Fine-tune

> Fine tuning is basically teaching a model to generalized. To generate data it has not seen before
> RAG is good for knowledge based expertise

>> Productionize
- Determine API between model and platform(s)
- Identify model hosting and deployment architecture
- Address scaling, monitoring, security, compliance, observability
- Evals: measure the business-focused metrics
- Continuously retrain and measure performance

=>> Four steps of Training
Tweaking the parameters of a model based on training data in a way that should generalize to unseen data
1. Forward pass: Predict the output given inputs
2. Loss calculation: How different was the prediction to the ground truth
3. Backward pass: How should we tweak parameters to do better next time (the "gradients")
4. Optimization: Update parameters a tiny step to do better next time

You can specify additional settings that aren't updated during training known as hyper-parameters like epoch and learning rate.

=>> Fine-Tuning
Three Stages to Fine-Tuning with OpenAI
Documentation
platform.openai.com - Model Optimization - Fine-tuning
1. Supervised fine-tuning
2. Direct preference optimization (DPO)
3. Reinforcement fine-tuning (RFT)
platform.openai.com/fine-tune

Key objectives of fine-tuning for frontier models
1. setting style or tone in a way that can't be achieved with prompting
2. improving the reliability of producing a type of output
3. correcting failures to follow complex prompts
4. handling edge cases
5. performing a new skill or task that's hard to articulate in a prompt

Fine-tune open-source LLM
=>> Introduction to LoRA & QLoRA for Fine-Tuning
High level explanation of LoRA (Low Rank Adaptor)
Step 1: Freeze the weights - we will not optimize them
Step 2: Select some layers to target, called "Target Modules"
Step 3: Create new "adaptor" matrices with lower dimensions, fewer parameters
Step 4: Apply these "adaptors" to the Target Modules to adjust them - and these get trained
There are two LoRA matrices that get applied
LoRA_a and LoRA_b (LoRA_a x LoRA_b to get a big matrix)
Finally you will fine-time smaller matrices

=> Three essential Hyperparameters for LoRA fine-tuning
1. r - the rank, or how many dimensions in the low-rank matrices. Start with 8, then double to 16, then 32, until diminishing returns
2. Alpha - a scaling factor that multiplies the lower rank matrices. Twice the value of r.
3. Target Modules - which layers of the neural network are adapted. Start by targeting the attention heads

=> QLoRA (Quantization - the Q in QLoRA)
Keep the number of weights but reduce their precision
>> Reduce to 8 bits, or even to 4 bits

Note:
1. 4 bits are interpreted as float, not int
2.You are quantizing the base model. The adaptor matrices (LoRA) are still 32 bit
>>> bitsandbytes a package that deals with quantization

Two types of models
1. Base models - trained to predict the next token given an input
2. Chat models or instruct variant

Five Important Hyper-parameters for QLoRA
1. Target Modules - which layers of the nueral network of the LoRA to target
    Mostly to the attention layers
2. r (LoRA rank) - The number of parameters in the LoRA matrix. It is usually set to 4 or 8.
3. alpha (LoRA alpha) - The scaling factor for the LoRA matrix. It is usually set to 16.
4. Quantization - The number of bits to quantize the base model.
5. Dropout (LoRA dropout) - The dropout rate for the LoRA matrix. It is usually set to 10% i.e 0.1.

Five Important Hyper-parameters for Training
1. Epochs  - The number of times to iterate through the dataset.
2. Batch Size - How many data to process in a batch. The number of samples to process at once.
    Usually in powers of 2 e. 2, 4, 8, 16 ...
3. Learning Rate - The step size used to update the model parameters during training.
4. Gradient accumulation - A technique used to prevent exploding gradients.
5. Optimizer - The algorithm used to update the model parameters based on the gradients.
    Adam - A popular optimizer that uses the first and second moments of the gradients to update the model parameters.
    SGD - A simple optimizer that updates the model parameters based on the gradients.
    RMSprop - A variant of the Adam optimizer that uses a moving average of the squared gradients to update the model parameters.
    Adagrad - A variant of the SGD optimizer that uses a learning rate that decreases with the number of iterations.
    Adadelta - A variant of the Adagrad optimizer that uses

TRL = Transformers Reinforcement Learning
