openrouter.ai
model GLM 4.7
usage: 
response = openrouter.chat.completions.create(model="z-ai/glm-4.7", messages=[{"role": "user", "content": "Tell a joke about cats."}])

Abstraction layers
Langchain vs LiteLLM

Langchain: 
- Langchain is a popular library for building AI applications.
 It provides a high-level interface for working with LLMs, making it easier to use and more accessible for developers. 
 Langchain supports a variety of LLMs, including OpenAI, Hugging Face, and Cohere. It also provides a variety of tools for working with LLMs, 
 such as prompt engineering, data preprocessing, and evaluation. Langchain is a good choice for building applications that require a high level of abstraction.

Usage
from langchain_openai import ChatOpenAI

llm =ChatOpenAI(model="gpt-5-mini")
response=llm.invoke(tell_a_joke)
display(Markdown(response))

LiteLLM  is a lightweight LLM library that supports a variety of LLMs, including OpenAI, Hugging Face, and Cohere.
It also provides a variety of tools for working with LLMs, such as prompt engineering, data preprocessing, and evaluation.

Usage
from litellm import completion
response = completion(model="openai/gpt-4.1", messages=[{"role": "user", "content": "tell_a_joke"}])
display(Markdown(response))

Gemini
gemini/gemini-2.5-flash-lite

Research prompt caching

Gradio from HuggingFace
Building UIs with Gradio
Gradio is an open-source Python library that allows you to build UIs for your machine learning models. 
It's designed to be easy to use and integrate into your existing projects.

Gradio Resources
https://www.gradio.app/guides/quickstart

=> Tools
- Allows frontier model to connect with external functions
- Richer responses by extending knowledge
- Ability to carry out actions within the applications
- Enhanced capabilities, like calculations

Common use cases
1. fetch data or add knowledge or context
2. take action, like booking a meeting
3. perform calculations
4. modify the UI
and two other ways to use tools that forms the basis of Agentic AI
1. a tool can be used to make another call to an LLM
2. A tool can be used to track a ToDo list and track progress towards a goal

Agents (Agentic AI)
2 definitions
-> LLM that controls the workflow
-> An LLM agent that runs tools in a loop to achieve a goal
Common features
-> Memory/persistence
-> Planning capabilities
-> Autonomy
-> LLM orchestration via Tools
-> Functionality via Tools

Types of Gradio UI
1. gr.interface is for standard, simple UIs
2. gr.ChatInterface is for standard Chatbot UIs
3. gr.Blocks is for custom UIs where you control the components and the callbacks

Hugging Face
Two parts of hugging Face
1. Hugging face platform - huggingface.co
    - Hugging face model hub (open source models of all shapes and sizes)
    - Hugging face datasets (a treasure trove of datasets)
    - Hugging face spaces (a platform for building and sharing ML apps)
        - apps, many built in gradio, including leaderboards

2. Hugging Face Libraries
- Hugging face has open source libraries and implements open source transformer models using 
 a. pytorch (most popular)
 b  tensorflow

Six Hugging face libraries
1. hub (its a python library that lets you connect to hugging face hub to download the models or datasets from hugging face)
2. datasets
3. transformers 
4. peft (Paremeter Efficient Fine-Tuning)
5. trl (Transformer Reinforcement Learning)
6. accelerate (its a python library that lets you train your models on multiple GPUs or TPUs)

Google Colab
https://colab.research.google.com/
https://colab.research.google.com/drive/1DjcrYDZldAXKJ08x1uYIVCtItoLPk1Wr?usp=sharing

Code on a powerful GPU box
- run a Jupyter notebook in the cloud with a powerful runtime
- collaborate with others
- integrate with other Google services

Runtimes
Change runtime type to T4 GPU
1. CPU based
2. Lower spec GPU for free or low-cost T4(15GB GPU RAM)
3. Higher spec GPU for resource intensive runs - A100(40GB)

Setting up Colab with Hugging Face and running a model
-> Connecting Hugging Face to Colab
Go to https://huggingface.co/
-> Click on your profile icon in the top right corner, profile
-> Click on "Settings"
-> Access Tokens
-> Click on "Create new token"
-> Select Write in Token type
-> Give it a name and click on "Create token"
-> Copy the token and paste it in the Colab notebook
-> In colab, click on secrets, add new secret
-> Name it HF_TOKEN

Accessing the secret key
from google.colab import userdata
userdata.get('secretName') where secretName is HF_TOKEN


Introduction to Hugging Face Pipelines for quick AI inference
-> The two API level of Hugging Face
1. Pipelines: Higher level APIs to carry out standard tasks incredibly quickly. Pipelines are for simple out-of-the-box inference task.
 E.g.
    1. Sentiment analysis
    2. Classifier
    3. Named Entity Recognition
    4. Question Answering
    5. Summaring
    6. Translation

2. Tokenizers and Models: Lower level APIs to provide the most power and control

Use Pipelines to generate content (text, image, audio)
Training and inference
1. Training is when you provide a model with data for it to adapt to get better at a task in the future. it does this by updating its internal settings - the parameters or weights of the model.
    If you are Training a model that's already had some training, the activity is called fine-tuning.
2. Inference is when you are working with a model that has already been trained. You are using that model to produce new outputs on new inputs, taking advantage of everthing it learned while it was being trained.
    Inference is also sometimes referred to as "Execution" or "Running a model".
    The pipelines API in HuggingFace is only for use for inference - running a model that has already been trained.

Using Pipelines from Hugging Face
Step 1: Create a pipeline function then call

my_pipeline = pipeline(task, model =xx, device =xx)

device = "cuda" for NVIDIA GPU like T4, mps on a mac

Step 2: Then call it as many times as you want

my_pipeline(input1)
my_pipeline(input2)

Tokenizers: How LLMs converts text to numbers
Tokenizers are a bit of code that maps between Text and Tokens for a particular model
-> Translates between Text, Tokens and IDs with encode() and decode() methods
-> Contains a dictionary that can include special tokens to signal information to LLM, like start of prompt
-> Contains special tokens like start of a prompt (number 10 can be configured to indicate the start of a prompt)

The Tokenenizers for key models
Meta - Llama 3.1
Microsoft's entrant - Phi
DeepSeekAI - DeepSeek 3.1
Leading open-source coding model - Qwen 2.5 Coder

Tokenizers in action: Encoding and Decoding with Llama3.1

Pytorch is a python library for writing neural networks.

=> Choosing the Right LLM for your task 
Compare the following features of an LLM
The basics 1
1. Open-source or closed
2. Chat/Reasoning/Hybrid
3. Release date and knowledge cut-off
4. Paremeters
5. Training tokens
6. Context window

The basics 2
1. Inference cost
2. Training cost
3. Build cost
4. Time to market
5. Rate limits
6. Speed
7. Latency
8. Licence

=> AI leaderboards
1. Artificial analysis (artificialanalysis.ai)
2. Vellum
3. Scale.com (SEAL leaderboards)
4. Hugging Face leaderboards

=> RAG (Retrieval Augmented Generation)
Build a knowledge base - a database of useful information
1. Search the knowledge base for relevant information
2. Use the relevant information to generate a response
RAG is a pattern for building LLM applications that use a knowledge base to generate responses.

=> Vector embeddings and Encoder LLMs: The foundation of RAG
Encoding LLMs and Vector embeddings

Auto-Encoding vs Auto-Regressive LLMs
- Auto-regressive LLMs predict a future token based on the previous tokens
- Auto-encoding produce output based on the full input

Auto-encoding LLMs
- Applications include Sentiment Analysis and Classification
- Also used to calculate "Vector Embeddings",  representing an input as a list of numbers i.e. a Vector
    Examples include BERT (Bidirectional Encoding Representations from Transformers) from Google and OpenAI Embeddings from OpenAI

Difference between Tokens and Vectors
Token is the numerical representation of the input,
while vectors are the numerical representations of those words and are outputs

These vectors mathematically represent the 'meaning' of an input
- can represent a character, a token, a word, an entire document, or something abstraction
- typically have hundreds, or thousands of dimensions
- represent an 'understanding' of the inputs; similar inputs are close to each other
- support 'vector math' like the famous example 
    "King - Man + Woman = Queen"

Vector embeddings is simply a set of numbers  that represent a word

=> The big idea behind RAG
Question <-> Code <-> Encoder (Encoding LLM)
Code <-> Vector Datastore
Code -> LLM
The question is encoded into a vector by the encoder, which is used to retrieve the most relevant information from the vector datastore.
Then the retrieved information is passed to the LLM to answer the question.

=> Introduction to Langchain and Vector Databases
LangChain
- Open Source framework launched in October 2022
- Provides a common framework for interfacing with many LLMs
- LangChain v1 released October 2025 with sifnificant changes

Pros
- Simplifies the creation of applications using LLMs (eg AI assistants, RAG, summarization)
- sifnificant adoption in enterprise

Cons
- As APIs for LLMs have matured, converged and simplified, the need for a unifying framework like LangChain has decreased - particulary with OpenAI endpoins
- LangChain is a more heavyweight abstraction layer than LiteLLM and others and some of its code can be 'legacy'

=> Breaking documents into chunks with LangChain

=> Encoder Models vs Vector Databases: OpenAI, BERT, Chroma & FAISS

Encoder/Embedding Models

The Encoder Model turns text into a Vector Embedding.
This is then stored in a Vector Database like Chroma

Encoder Models
1. word2vec  (2013)
2. BERT (2018) from google

Modern Embedding Models
3. OpenAI
    - text embedding 3 small
    - text embedding 3 large
4. Google
    - gemini-embedding-001
5. Hugging Face Sentence Transformers
    - all-MiniLM-L6-v2

=> Use LangChain to load our Knowledge Base
- Read in the documents in all folders
- Divide the documents into chunks
- Store the chunks in a Vector Database
- Create a Retrieval Chain to query the Vector Database

=> Popular Vectorestores
Open-Source: Chroma, Qdrant, FAISS (in-memory)
Paid & scalable: Pinecone, Weaviate, etc
Mainstream databases (Postgres, MongoDB, Elastic)

=> Creating Vector Stores with Chroma and visualizing embeddings with t-SNE (t-distributed stochastic neighbor embedding)
=> Vector Visualizations and Comparing Embedding Models

=> Building a Complete RAG pipeline with LangChain and Chroma

Two key abstractions
1. LLm 
2. Retrieval
Many common abstraction like this can be called with invoke()

temperature as a parameter
temperature is a parameter that controls the randomness of the output. It is a value between 0 and 1, where 0 is deterministic and 1 is completely random. 
A higher temperature will result in more diverse outputs, while a lower temperature will result in more focused outputs.

=> Building RAG with LangChain: Retriever and LLM Integration
=> Building Production RAG with Python Modules and Gradio UI
=> RAG with Conversation History: Building a Gradio UI and Debugging Chunki

=>> Convert Documents Into LLM-Ready Markdown
1. MarkItDown library
2. Pandoc  library

See materials-python-markitdown for more details.