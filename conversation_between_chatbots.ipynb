{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb5759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e5277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load  environment variables\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d32f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Check the openai key\n",
    "if not openai_api_key:\n",
    "    print(\"OpenAI API key not found.\")\n",
    "elif openai_api_key[:8] !=\"sk-proj-\":\n",
    "    print(\"Invalid OpenAI API key\")\n",
    "elif openai_api_key.strip() != openai_api_key:\n",
    "    print(\"OpenAI API key contains whitespace\")\n",
    "else:\n",
    "    print(\"OpenAI API key found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11adef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check if Ollama is running by making a proper request\n",
    "\n",
    "try:\n",
    "    response = requests.get(\"http://localhost:11434\")\n",
    "    print(f\"Status Code: {response.status_code}\")\n",
    "    print(f\"Response: {response.text}\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Cannot connect to Ollama. Make sure it's running on localhost:11434\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bca746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an instance of openai\n",
    "\n",
    "openai =OpenAI(api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declarations\n",
    "OLLAMA_BASE_URL  = \"http://localhost:11434/v1\"\n",
    "\n",
    "ollama = OpenAI(base_url=OLLAMA_BASE_URL, api_key=\"ollama\")\n",
    "\n",
    "ollama_model = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abbcd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "ollama_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f74c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845947bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ollama():\n",
    "    messages = []\n",
    "    for gpt, ollama_message in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ollama_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    message= ollama.chat.completions.create(model=ollama_model,\n",
    "                                          messages=messages)\n",
    " \n",
    "    return message.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4b05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Ollama:\\n{ollama_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    ollama_next = call_ollama()\n",
    "    print(f\"Ollama:\\n{ollama_next}\\n\")\n",
    "    ollama_messages.append(ollama_next)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM Engineer",
   "language": "python",
   "name": "llm-engineer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
